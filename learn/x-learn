#!/usr/bin/python3
import sklearn #http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics
from sklearn.neighbors import *    
from sklearn.svm import *          
from sklearn.linear_model import * 
from sklearn.naive_bayes import *  
from sklearn.tree import *         
from sklearn.ensemble import *     
from sklearn.cluster import *
from scipy import sparse
from sklearn.externals import joblib
from sklearn import metrics         

from fcmeans import FCM
from collections import defaultdict
import numpy as np
import pickle
import argparse
import sys
import types

models = {
        'linearr'   :LinearRegression,
        'logisticr' :LogisticRegression,
        'knnc'      :KNeighborsClassifier,
        'svc'       :SVC,
        'lsvc'      :LinearSVC,
        'sgdc'      :SGDClassifier,
        'dtc'       :DecisionTreeClassifier,
        'abc'       :AdaBoostClassifier,
        'etc'       :ExtraTreesClassifier,
        'rfc'       :RandomForestClassifier,
        'gbc'       :GradientBoostingClassifier,
        
        'knnr'      :KNeighborsRegressor,
        'svr'       :SVR,
        'nusvr'     :NuSVR,
        'dtr'       :DecisionTreeRegressor,
        'rfr'       :RandomForestRegressor,
        'etr'       :ExtraTreesRegressor,
        'abr'       :AdaBoostRegressor,
        'gbr'       :GradientBoostingRegressor,
        
        'perceptron':Perceptron,
        'ridge'     :Ridge,
        'lasso'     :Lasso,
        'elasticnet':ElasticNet,
        'mnb'       :MultinomialNB,
        'bnb'       :BernoulliNB,
        
        'agl'       :AgglomerativeClustering,
        'msl'       :MeanShift,
        'apl'       :AffinityPropagation,
        'dbscanl'   :DBSCAN,
#        'ol'        :OPTICS,
        'fal'       :FeatureAgglomeration,
        'scl'       :SpectralClustering,
        'mbkl'      :MiniBatchKMeans,
        'kml'       :KMeans,
        'sbl'       :SpectralBiclustering,
        'scl'       :SpectralCoclustering,
        'bl'        :Birch,
        'fcml'      :FCM,
        }
#cluster.AgglomerativeClustering ([?])                              # Agglomerative Clustering
#cluster.MeanShift               ([bandwidth, seeds, ?])            # Mean shift clustering using a flat kernel.
#cluster.AffinityPropagation     ([damping, ?])                     # Perform Affinity Propagation Clustering of data.
#cluster.DBSCAN                  ([eps, min_samples, metric, ?])    # Perform DBSCAN clustering from vector array or distance matrix.
#cluster.OPTICS                  ([min_samples, max_eps, ?])        # Estimate clustering structure from vector array.
#cluster.FeatureAgglomeration    ([n_clusters, ?])                  # Agglomerate features.
#cluster.SpectralClustering      ([n_clusters, ?])                  # Apply clustering to a projection of the normalized Laplacian.
#cluster.MiniBatchKMeans         ([n_clusters, init, ?])            # Mini-Batch K-Means clustering.
#cluster.KMeans                  ([n_clusters, init, n_init, ?])    # K-Means clustering.
#cluster.SpectralBiclustering    ([n_clusters, ?])                  # Spectral biclustering (Kluger, 2003).
#cluster.SpectralCoclustering    ([n_clusters, ?])                  # Spectral Co-Clustering algorithm (Dhillon, 2001).
#cluster.Birch                   ([threshold, branching_factor, ?]) # Implements the Birch clustering algorithm.

sparse_models = set([
    SVR,
    NuSVR,
    LinearSVC,
    KNeighborsClassifier,
    KNeighborsRegressor,
    SGDClassifier,
    Perceptron,
    Ridge,
    LogisticRegression,
    LinearRegression,
])
args = []
def format_table(table):
    if len(table) == 0:
        return ''
    col_length = defaultdict(int)
    for row in table:
        for ind, item in enumerate(row):
            col_length[ind] = max(col_length[ind], len(item))
    ret = ''
    for row in table:
        for ind, item in enumerate(row):
            fmtstr = '{{:<{}}}' . format(col_length[ind])
            ret += fmtstr.format(item) + " "
        ret += "\n"
    return ret
def get_model_abbr_help():
    lines = format_table([['Abbreviation', 'Model']] + map(lambda item: [item[0], item[1].__name__],  sorted(models.items()))).split('\n')
    return "\n".join(map(lambda x: ' ' * 8 + x, lines))
class VerboseAction(argparse.Action):
    def __call__(self, parser, args, values, option_string=None):
        if values==None:
            values='1'
        try:
            values=int(values)
        except ValueError:
            values=values.count('v')+1
        setattr(args, self.dest, values)
def get_args():
    description = 'command line wrapper for some models in scikit-learn'
    tasks = ['fit', 'predict', 'fitpredict',
            'f', 'p', 'fp',
            'doc']
    task_arg_setting = [
            (['fit', 'f'],
                ['training_file', 'model', 'model_output'],
                ['model_options']),
            (['predict', 'p'],
                ['test_file', 'model_input', 'prediction_file'],
                []),
            (['fitpredict', 'fp'],
                ['training_file', 'model', 'test_file', 'prediction_file'],
                ['model_options', 'model_output']),
            (['doc'],
                ['model'],
                [])
            ]
    epilog = "task specification:\n{}" . format(
            "\n" . join([
                '    task name: {}\n        required arguments: {}\n        optional arguments: {}' . format(
                    *map(lambda item: ", " . join(item), setting))  for setting in task_arg_setting]))
    epilog += "\n"
    epilog += "Notes:\n"
    epilog += "    1. model abbreviation correspondence:\n"
    epilog += get_model_abbr_help() + '\n'
    epilog += "    2. model compatible with sparse matrix:\n"
    epilog += ' ' * 8 + ", " . join(map(lambda x: x.__name__, sparse_models))
    epilog += '\n'
    epilog += '\n'
    epilog += 'Examples:\n'
    epilog += """ 1. fit(train) a SVR model with sigmoid kernel:
        ./learner.py -t f --training-file training-data --model svr \ --model-output model.svr kernel:s:sigmoid
    2. predict using precomputed model:
        ./learner.py -t p --test-file test --model-input model.svr
            --prediction-file pred-result
    3. fit and predict, model saved, verbose output, and show metrics:
        ./learner.py -t fp --training-file training-data --model svr \ --model-output model.svr --test-file test-data \ --prediction-file pred-result -v --show-metrics
    4. pass parameters for svc model, specify linear kernel:
        ./learner.py --task fp --training-file training-data --model svc \ --test-file test-data --prediction-file pred-result \ --show-metrics kernel:s:linear
    5. show documents:
        ./learner.py -t doc --model svc
"""
    parser = argparse.ArgumentParser(
            description = description, epilog = epilog,
            formatter_class = argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-t', '--task',
            choices = tasks,
            help = 'task to process, see help for detailed information',
            required = True)
    parser.add_argument('-i','--training-file',
            help = 'input: training file, svm format by default')
    parser.add_argument('-e','--test-file',
            help = 'input: test file, svm format by default')
    parser.add_argument('-mi','--model-input',
            help = 'input: model input file, used in prediction')
    parser.add_argument('-mo','--model-output',
            help = 'output: model output file, used in fitting')
    parser.add_argument('-m', '--model',
            help = 'model, specified in fitting',
            choices = models)
    parser.add_argument('-o','--prediction-file',
            help = 'output: prediction file')
    parser.add_argument('--model-format',
            choices = ['pickle', 'joblib'],
            default = 'pickle',
            help = 'model format, pickle(default) or joblib')
    parser.add_argument('-v','--show-metrics',
            action = 'store_true',
            help = 'show metric after prediction')
    parser.add_argument('-V', '--verbose',
            help = 'verbose level, -v <level> or multiple -v\'s or something like -vvv',
            nargs = '?',
            default = 0,
            action = VerboseAction)
    parser.add_argument('model_options',
            nargs = '*',
            help = """ additional paramters for specific model of format "name:type:val",  effective only when training is needed. type is either int, float or str,  which abbreviates as i, f and s.""")
    args = parser.parse_args()
    model_options = dict()
    for opt in args.model_options:
        opt = opt.split(':')
        assert len(opt) == 3, 'model option format error'
        key, t, val = opt
        if t == 'i':
            t = 'int'
        elif t == 'f':
            t = 'float'
        elif t == 's':
            t = 'str'
        elif t == 'b':
            t = 'bool'
        model_options[key] = eval(t)(val)
    args.model_options = model_options
    for setting in task_arg_setting:
        if args.task in setting[0]:
            args.task = setting[0][0]
    def check_params(task, argnames):
        if args.task in task:
            for name in argnames:
                if not(name in args.__dict__ and args.__dict__[name]):
                    info = 'argument `{}\' must present in `{}\' task' . format("--" + name.replace('_', '-'), task[0])
                    raise Exception(info)
    try:
        for setting in task_arg_setting:
            check_params(setting[0], setting[1])
    except Exception as e:
        sys.stderr.write(str(e) + '\n')
        sys.exit(1)
    def verbose_print(self, msg, vb = 1): 
        if vb <= self.verbose:
            print(msg)
    args.vprint = types.MethodType(verbose_print, args, args.__class__)
    return args
def read_svmformat_data(fname):
    import pandas as pd
    import numpy as np
    data = pd.read_csv(fname)
    X=data.iloc[:,:-1]
    y=data.iloc[:,-1:]
    return np.array(X), np.array(y)
def write_labels(fname, y_pred):
    count_types = defaultdict(int)
    for y in y_pred:
        count_types[type(y)] += 1
    most_prevalent_type = sorted(map(lambda x: (x[1], x[0]), count_types.iteritems()))[0][1]
    if most_prevalent_type == float:
        typefmt = '{:f}'
    else:
        typefmt = '{}'
    with open(fname, 'w') as fout:
        for y in y_pred:
            fout.write(typefmt.format(y) + '\n')
def get_model(args):
    model = models[args.model](**args.model_options)
    return model
def get_dim(X):
    dim = -1
    for x in X:
        for ind, val in x:
            if ind > dim:
                dim = ind
    return dim + 1
def preprocess_data(model, X):
    if model in sparse_models:
        return X
    return X
def save_model(fname, model):
    if args.model_format == 'pickle':
        fd = open(fname, 'wb')
        pickle.dump(model, fd)
        fd.close()
    else:
        joblib.dump(model, fname)
def load_model(fname):
    if args.model_format == 'pickle':
        fd = open(fname, 'rb')
        model = pickle.load(fd)
        fd.close()
        return model
    else:
        return joblib.load(fname)
def show_metrics(args, y_true, y_pred):
    if (args.model.endswith('c')):
      # Classification
      #metrics.precision_recall_fscore_support(?)                         # Compute precision, recall, F-measure and support for each class
      #metrics.auc                            (x, y)                      # Compute Area Under the Curve(AUC) using the trapezoidal rule
      #metrics.average_precision_score        (y_true, y_score)           # Compute average precision(AP) from prediction scores
      #metrics.dcg_score                      (y_true, y_score[, k, ?])   # Compute Discounted Cumulative Gain.
      #metrics.roc_auc_score                  (y_true, y_score[, ?])      # Compute Area Under the Receiver Operating Characteristic Curve(ROC AUC) from prediction scores.
      #etrics.roc_curve                       (y_true, y_score[, ?])      # Compute Receiver operating characteristic(ROC)
      #metrics.ndcg_score                     (y_true, y_score[, k, ?])   # Compute Normalized Discounted Cumulative Gain.
      #metrics.hinge_loss                     (y_true, pred_decision[, ?])# Average hinge loss(non-regularized)
      #metrics.fbeta_score                    (y_true, y_pred, beta[, ?]) # Compute the F-beta score
      #metrics.cohen_kappa_score              (y1, y2[, labels, ?])       # Cohen's kappa: a statistic that measures inter-annotator agreement.
      #metrics.brier_score_loss               (y_true, y_prob[, ?])       # Compute the Brier score.
      #metrics.multilabel_confusion_matrix    (y_true)                    # Compute a confusion matrix for each class or sample
      #metrics.precision_recall_curve         (y_true)                    # Compute precision-recall pairs for different probability thresholds
      #metrics.precision_score                (y_true, y_pred)            # Compute the precision
      #metrics.recall_score                   (y_true, y_pred)            # Compute the recall
      #metrics.f1_score                       (y_true, y_pred)            # Compute the F1 score, also known as balanced F-score or F-measure
      #metrics.jaccard_score                  (y_true, y_pred)            # Jaccard similarity coefficient score
      #metrics.log_loss                       (y_true, y_pred)            # Log loss, aka logistic loss or cross-entropy loss.
      print "Classification metrics:"
      print "confusion_matrix =\n"      ,metrics.confusion_matrix               (y_true, y_pred)            # Compute confusion matrix to evaluate the accuracy of a classification.
      print "accuracy_score ="          ,metrics.accuracy_score                 (y_true, y_pred)            # Accuracy classification score.
      print "balanced_accuracy_score =" ,metrics.balanced_accuracy_score        (y_true, y_pred)            # Compute the balanced accuracy
      print "classification_report ="   ,metrics.classification_report          (y_true, y_pred)            # Build a text report showing the main classification metrics
      print "hamming_loss ="            ,metrics.hamming_loss                   (y_true, y_pred)            # Compute the average Hamming loss.
      print "matthews_corrcoef ="       ,metrics.matthews_corrcoef              (y_true, y_pred)            # Compute the Matthews correlation coefficient(MCC)
      print "zero_one_loss ="           ,metrics.zero_one_loss                  (y_true, y_pred)            # Zero-one classification loss.

    if (args.model.endswith('r')):
      # Regression
      #metrics.mean_poisson_deviance   (y_true, y_pred) # Mean Poisson deviance regression loss.
      #metrics.mean_gamma_deviance     (y_true, y_pred) # Mean Gamma deviance regression loss.
      #metrics.mean_tweedie_deviance   (y_true, y_pred) # Mean Tweedie deviance regression loss.
      #metrics.max_error               (y_true, y_pred) # max_error metric calculates the maximum residual error.
      print "Regression metrics:"
      print "explained_variance_score"  ,metrics.explained_variance_score(y_true, y_pred) # Explained variance regression score function
      print "mean_absolute_error"       ,metrics.mean_absolute_error     (y_true, y_pred) # Mean absolute error regression loss
      print "mean_squared_error"        ,metrics.mean_squared_error      (y_true, y_pred) # Mean squared error regression loss
      print "mean_squared_log_error"    ,metrics.mean_squared_log_error  (y_true, y_pred) # Mean squared logarithmic error regression loss
      print "median_absolute_error"     ,metrics.median_absolute_error   (y_true, y_pred) # Median absolute error regression loss
      print "r2_score"                  ,metrics.r2_score                (y_true, y_pred) # R^2 (coefficient of determination) regression score function.

    # Clustering
    #metrics.homogeneity_completeness_v_measure(?)                   # Compute the homogeneity and completeness and V-Measure scores at once.
    #metrics.cluster.contingency_matrix        (?[, ?])              # Build a contingency matrix describing the relationship between labels.
    #metrics.adjusted_mutual_info_score        (?[, ?])              # Adjusted Mutual Information between two clusterings.
    #metrics.normalized_mutual_info_score      (?[, ?])              # Normalized Mutual Information between two clusterings.
    #metrics.v_measure_score                   (y_true, labels_pred) # V-measure cluster labeling given a ground truth.
    #metrics.adjusted_rand_score               (y_true)              # Rand index adjusted for chance.
    #metrics.completeness_score                (y_true)              # Completeness metric of a cluster labeling given a ground truth.
    #metrics.fowlkes_mallows_score             (y_true)              # Measure the similarity of two clusterings of a set of points.
    #metrics.homogeneity_score                 (y_true)              # Homogeneity metric of a cluster labeling given a ground truth.
    #metrics.mutual_info_score                 (y_true)              # Mutual Information between two clusterings.
    #metrics.silhouette_score                  (X, labels[, ?])      # Compute the mean Silhouette Coefficient of all samples.
    #metrics.silhouette_samples                (X, labels[, metric]) # Compute the Silhouette Coefficient for each sample.
    #metrics.calinski_harabasz_score           (X, labels)           # Compute the Calinski and Harabasz score.
    #metrics.davies_bouldin_score              (X, labels)           # Computes the Davies-Bouldin score.

    # Model selection
    #X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=5)
    #model_selection.permutation_test_score(?)                # Evaluate the significance of a cross-validated score with permutations
    #model_selection.cross_validate        (estimator, X)     # Evaluate metric                                               (s) by cross-validation and also record fit/score times.
    #model_selection.cross_val_predict     (estimator, X)     # Generate cross-validated estimates for each input data point     
    #model_selection.cross_val_score       (estimator, X)     # Evaluate a score by cross-validation                           
    #model_selection.learning_curve        (estimator, X, y)  # Learning curve.                                               
    #model_selection.validation_curve      (estimator)        # Validation curve.                                               
    #print("Accuracy: ",                 round(model_selection.cross_val_score(classifier_gaussiannb, X, y, scoring="accuracy", cv=num_validations).mean(), 2))
    #print("F1: ",                       round(model_selection.cross_val_score(classifier_gaussiannb, X, y, scoring="f1_weighted", cv=num_validations).mean(), 2))
    #print("Precision: ",                round(model_selection.cross_val_score(classifier_gaussiannb, X, y, scoring="precision_weighted", cv=num_validations).mean(), 2))
    #print("Recall: ",                   round(model_selection.cross_val_score(classifier_gaussiannb, X, y, scoring="recall_weighted", cv=num_validations).mean(), 2))

#    # Pairwaise
#    metrics.pairwise.distance_metrics          ()                 # Valid metrics for pairwise_distances.
#    metrics.pairwise.kernel_metrics            ()                 # Valid metrics for pairwise_kernels
#    metrics.pairwise.nan_euclidean_distances   (X)                # Calculate the euclidean distances in the presence of missing values.
#    metrics.pairwise.cosine_distances          (X[, Y])           # Compute cosine distance between samples in X and Y.
#    metrics.pairwise.cosine_similarity         (X[, Y, ?])        # Compute cosine similarity between samples in X and Y.
#    metrics.pairwise_distances_argmin_min      (X, Y)             # Compute minimum distances between one point and a set of points.
#    metrics.pairwise_distances_argmin          (X, Y[, ?])        # Compute minimum distances between one point and a set of points.
#    metrics.pairwise.additive_chi2_kernel      (X[, Y])           # Computes the additive chi-squared kernel between observations in X and Y
#    metrics.pairwise.paired_cosine_distances   (X, Y)             # Computes the paired cosine distances between X and Y
#    metrics.pairwise.paired_euclidean_distances(X, Y)             # Computes the paired euclidean distances between X and Y
#    metrics.pairwise.haversine_distances       (X[, Y])           # Compute the Haversine distance between samples in X and Y
#    metrics.pairwise.pairwise_kernels          (X[, Y, ?])        # Compute the kernel between arrays X and optional array Y.
#    metrics.pairwise.paired_manhattan_distances(X, Y)             # Compute the L1 distances between the vectors in X and Y.
#    metrics.pairwise.manhattan_distances       (X[, Y, ?])        # Compute the L1 distances between the vectors in X and Y.
#    metrics.pairwise.linear_kernel             (X[, Y, ?])        # Compute the linear kernel between X and Y.
#    metrics.pairwise.polynomial_kernel         (X[, Y, ?])        # Compute the polynomial kernel between X and Y.
#    metrics.pairwise.sigmoid_kernel            (X[, Y, ?])        # Compute the sigmoid kernel between X and Y.
#    metrics.pairwise.euclidean_distances       (X[, Y, ?])        # Considering the rows of X (and Y=X) as vectors, compute the distance matrix between each pair of vectors.
#    metrics.pairwise.chi2_kernel               (X[, Y, gamma])    # Computes the exponential chi-squared kernel X and Y.
#    metrics.pairwise.laplacian_kernel          (X[, Y, gamma])    # Compute the laplacian kernel between X and Y.
#    metrics.pairwise.rbf_kernel                (X[, Y, gamma])    # Compute the rbf (gaussian) kernel between X and Y.
#    metrics.pairwise_distances_chunked         (X[, Y, ?])        # Generate a distance matrix chunk by chunk with optional reduction
#    metrics.pairwise.paired_distances          (X, Y[, metric])   # Computes the paired distances between X and Y.
#    metrics.pairwise_distances                 (X[, Y, metric, ?])# Compute the distance matrix from a vector array X and optional Y.

def get_labels(algorithm, X):
    if hasattr(algorithm, "labels_"):
          labels = algorithm.labels_.astype(np.int)
    elif hasattr(algorithm, "row_labels_"):
          labels = algorithm.row_labels_.astype(np.int)
    else:
          labels = algorithm.predict(X)
    num_clusters = len(np.unique(labels))
    if hasattr(algorithm, "cluster_centers_"):
          cluster_centerx = algorithm.cluster_centers_
    elif hasattr(algorithm, "affinity_matrix_"):
          cluster_centerx = algorithm.affinity_matrix_
    elif hasattr(algorithm, "components_"):
          cluster_centerx = algorithm.components_
    elif hasattr(algorithm, "centers"):
          cluster_centerx = algorithm.centers
    else:
        cluster_centerx=np.zeros((num_clusters,len(X[1])))
#    timex=t1-t0
    return labels, cluster_centerx


def task_fit(args):
    model = get_model(args)
    args.vprint('reading training file {} ...' . format(args.training_file))
    X_train, y_train = read_svmformat_data(args.training_file)
    args.vprint('preprocessing training data ...')
    X_train = preprocess_data(model, X_train)
    args.vprint('training model {} ...' . format(model.__class__.__name__))
    if (args.model.endswith('l')):
        model.fit(np.hstack((X_train,y_train)))
    else:
        model.fit(X_train, y_train)
    args.vprint('saving model ...')
    save_model(args.model_output, model)
def task_predict(args):
    model = load_model(args.model_input)
    args.vprint('reading test file {} ...' . format(args.test_file))
    X_test, y_test = read_svmformat_data(args.test_file)
    args.vprint('preprocessing test data ...')
    X_test = preprocess_data(model, X_test)
    args.vprint('predicting ...')
    y_pred, centers = get_labels(model, X_test)
    args.vprint('writing predictions ...')
    write_labels(args.prediction_file, y_pred)
    if args.show_metrics:
        show_metrics(args, y_test, y_pred)
def task_fitpredict(args):
    model = get_model(args)
    args.vprint('reading training file {} ...' . format(args.training_file))
    X_train, y_train = read_svmformat_data(args.training_file)
    args.vprint('reading test file {} ...' . format(args.test_file))
    X_test, y_test = read_svmformat_data(args.test_file)
    args.vprint('preprocessing training and test data ...')
    X_train = preprocess_data(model, X_train)
    X_test = preprocess_data(model, X_test)
    args.vprint('training model {} ...' . format(model.__class__.__name__))
    if (args.model.endswith('l')):
        model.fit(np.hstack((X_train,y_train)))
    else:
        model.fit(X_train, y_train)
    args.vprint('predicting ...')
    y_pred, centers = get_labels(model, X_test)
    if args.model_output:
        args.vprint('saving model ...')
        save_model(args.model_output, model)
    args.vprint('writing predictions ...')
    write_labels(args.prediction_file, y_pred)
    write_labels(args.prediction_file + "u", centers)
    if args.show_metrics:
        show_metrics(args, y_test, y_pred)
def main():
    global args
    args = get_args()
    if args.task == 'doc':
        print(models[args.model].__doc__)
    else:
        task_worker = dict(
                fit = task_fit,
                predict = task_predict,
                fitpredict = task_fitpredict)
        task_worker[args.task](args)
if __name__ == '__main__':
    main()
